import pickle
from datetime import datetime
from sklearn.linear_model import LinearRegression

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from matplotlib.ticker import FuncFormatter
from pprint import pprint
from sklearn.metrics import r2_score, mean_squared_error

from main import create_all_features, abc_transform, append_to_csv

plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")


def add_abc(abc_params_dict, data_wo_abc):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç ABC –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º
    abc_params_dict: —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞ {'channel_A': value, 'channel_B': value, 'channel_C': value}
    data_wo_abc: DataFrame —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    channels = set()
    for key in abc_params_dict.keys():
        if key.endswith(('_A', '_B', '_C')):
            channel = key.rsplit('_', 1)[0]  # –£–±–∏—Ä–∞–µ–º _A, _B, _C
            channels.add(channel)

    applied_params = {}

    for channel in channels:
        a_key = f'{channel}_A'
        b_key = f'{channel}_B'
        c_key = f'{channel}_C'

        if all(key in abc_params_dict for key in [a_key, b_key, c_key]):
            A = abc_params_dict[a_key]
            B = abc_params_dict[b_key]
            C = abc_params_dict[c_key]

            print(f'{channel}_abc', 'qq')

            # –ò—â–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–ª–æ–Ω–∫—É (—É–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã)
            original_feature = channel.lower()

            if original_feature in data_wo_abc.columns:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º ABC –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
                original_values = data_wo_abc[original_feature].values
                abc_transformed = abc_transform(original_values, A, B, C)
                feature_name = f'{original_feature}_abc'
                print(feature_name, 'feature_name')
                data_wo_abc[feature_name] = abc_transformed
                applied_params[a_key] = A
                applied_params[b_key] = B
                applied_params[c_key] = C

            else:
                print(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–∏—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {original_feature}")

    return applied_params, data_wo_abc


def normalize_feature_name(feature_name):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
    """
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –∑–∞–ø—è—Ç—ã–µ, —Ç–æ—á–∫–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = (feature_name.lower()
                  .replace(' ', '_')
                  .replace(',', '')
                  .replace('.', '')
                  .replace('(', '')
                  .replace(')', ''))
    return normalized


class MMMVisualizer:
    def __init__(self, coef, features, abc_params, train_data):
        """
        results_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ features, p-values, coef, A, B, C
        """
        self.train_data = train_data
        self.coef = np.array(coef)
        features = list(map(lambda x: x.lower(), features))
        self.features = features
        self.abc_params = abc_params

    def plot_saturation_curves(self, save_path='saturation_curves.png'):
        """
        –°—Ç—Ä–æ–∏—Ç –∫—Ä–∏–≤—ã–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏
        """
        model_abc_params = self.abc_params

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        real_media_data = self.get_real_media_levels()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –∏–∑ –∫–ª—é—á–µ–π
        channels = set()
        for key in model_abc_params.keys():
            if key.endswith(('_A', '_B', '_C')):
                channel = key.rsplit('_', 1)[0]
                channels.add(channel)

        media_channels = []
        for channel in channels:
            a_key = f'{channel}_A'
            b_key = f'{channel}_B'
            c_key = f'{channel}_C'
            if all(key in model_abc_params for key in [a_key, b_key, c_key]):
                media_channels.append({
                    'name': channel,
                    'A': model_abc_params[a_key],
                    'B': model_abc_params[b_key],
                    'C': model_abc_params[c_key]
                })

        if len(media_channels) == 0:
            return

        n_channels = len(media_channels)
        cols = min(3, n_channels)
        rows = (n_channels + cols - 1) // cols

        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
        if n_channels == 1:
            axes = [axes]
        elif rows == 1:
            axes = [axes] if cols == 1 else list(axes)
        else:
            axes = axes.flatten()

        fig.suptitle('–ö—Ä–∏–≤—ã–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è –º–µ–¥–∏–∞-–∫–∞–Ω–∞–ª–æ–≤', fontsize=16, fontweight='bold')

        for idx, channel_data in enumerate(media_channels):
            ax = axes[idx]
            A = channel_data['A']
            B = channel_data['B']
            C = channel_data['C']
            channel_name = channel_data['name'].replace('_', ' ').title()
            original_channel = self.map_abc_to_original_channel(channel_data['name'])

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if original_channel in real_media_data:
                max_historical = real_media_data[original_channel]['max']
                mean_spend = real_media_data[original_channel]['mean']
                base_max = max_historical * 2.5
            else:
                mean_spend = 25
                base_max = 100

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            test_range = np.linspace(0, base_max * 2, 500)

            # ABC —Ñ—É–Ω–∫—Ü–∏—è
            def abc_curve(spend, a, b, c):
                if isinstance(spend, (int, float)):
                    spend = np.array([spend])

                adstocked = np.zeros_like(spend, dtype=float)
                adstocked[0] = spend[0]
                for i in range(1, len(spend)):
                    adstocked[i] = spend[i] + a * adstocked[i - 1]

                saturated = (c * adstocked) / (1 + c * adstocked + 1e-10)
                final = b * saturated
                return final

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—Ä–∏–≤—É—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ—á–∫–∏ –æ–±—Ä–µ–∑–∞–Ω–∏—è
            test_response = abc_curve(test_range, A, B, C)
            derivatives = np.gradient(test_response, test_range)

            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫—É –≥–¥–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –ø–∞–¥–∞–µ—Ç –¥–æ 5% –æ—Ç –º–∞–∫—Å–∏–º—É–º–∞
            max_derivative = np.max(derivatives[1:20])
            significant_growth_threshold = max_derivative * 0.05

            # –¢–æ—á–∫–∞ –≥–¥–µ —Ä–æ—Å—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º
            low_growth_indices = np.where(derivatives < significant_growth_threshold)[0]

            if len(low_growth_indices) > 20:
                cutoff_idx = low_growth_indices[0]
                optimal_max = test_range[cutoff_idx] * 1.3
            else:
                optimal_max = base_max

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
            final_max = min(optimal_max, base_max)

            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            spend_range = np.linspace(0, final_max, 200)
            response = abc_curve(spend_range, A, B, C)

            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∏–≤–æ–π
            ax.plot(spend_range, response, linewidth=3, color='#2E86AB', label='–ö—Ä–∏–≤–∞—è –æ—Ç–∫–ª–∏–∫–∞')
            ax.fill_between(spend_range, 0, response, alpha=0.3, color='#2E86AB')

            # –¢–æ–ª—å–∫–æ —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å (–∂–µ–ª—Ç—ã–π —à–∞—Ä–∏–∫)
            mean_response = abc_curve(np.array([mean_spend]), A, B, C)[0]
            ax.scatter([mean_spend], [mean_response], color='orange', s=100,
                       zorder=5, label=f'–°—Ä–µ–¥–Ω–∏–π: {mean_spend:.0f}')

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –ø–æ–¥–ø–∏—Å–∏
            ax.set_title(f'{channel_name}\nA={A:.2f}, B={B:.2f}, C={C:.2f}',
                         fontweight='bold', pad=20)
            ax.set_xlabel('–ú–µ–¥–∏–∞-–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', fontweight='bold')
            ax.set_ylabel('–ü—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–¥–∞–∂', fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend(fontsize=9)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
        for idx in range(n_channels, len(axes)):
            axes[idx].remove()

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # plot_saturation_curves(model_abc_params)

    def create_model_from_coefficients(self, train_data, forecast_data, abc_params, data_to_plot,
                                       target_col='sales', intercept=None, save_path=None):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∏ ABC –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Args:
            original_data: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (DataFrame)
            coefficients_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ features, p-values, coef, A, B, C
            target_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            intercept: —Å–≤–æ–±–æ–¥–Ω—ã–π —á–ª–µ–Ω (–µ—Å–ª–∏ None, —Ä–∞—Å—Å—á–∏—Ç–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            dict: model_data —Å –º–æ–¥–µ–ª—å—é, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        features = self.features
        print(train_data.columns)
        print(features)
        missing_features = [f for f in features if f not in train_data.columns]

        if missing_features:
            print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
            return None

        clean_data = train_data[features + [target_col]].dropna()

        clean_data.to_csv('test.csv', index=False)

        X = clean_data[features].values
        y = clean_data[target_col].values
        coefficients = self.coef

        print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(clean_data)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        model = LinearRegression()
        model.coef_ = coefficients
        if intercept is None:
            predicted_without_intercept = X @ coefficients
            intercept = np.mean(y) - np.mean(predicted_without_intercept)

        model.intercept_ = intercept
        model.n_features_in_ = len(features)
        model.feature_names_in_ = np.array(features)

        y_pred = model.predict(X)
        r2_score_ = r2_score(y, y_pred)

        mse = mean_squared_error(y, y_pred)
        rmse = np.sqrt(mse)

        print(f"–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   R¬≤: {r2_score_:.4f}")
        print(f"   RMSE: {rmse:.2f}")

        params = abc_params.copy()
        used_media = [f for f in features if '_abc' in f]
        used_price = [f for f in features if any(p in f.lower() for p in ['price', 'premium', 'discount'])]
        used_seasonal = [f for f in features if
                         any(s in f.lower() for s in ['spring', 'summer', 'autumn', 'winter', 'holiday', 'trend'])]
        used_other = [f for f in features if f not in used_media + used_price + used_seasonal]
        params.update({
            'used_media': used_media,
            'used_price': used_price,
            'used_seasonal': used_seasonal,
            'used_other': used_other,
            'intercept': intercept
        })

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metadata = {
            'timestamp': timestamp,
            'creation_method': 'from_coefficients',
            'n_features': len(features),
            'target_col': target_col,
            'train_data_shape': clean_data.shape,
            'r2_score': r2_score_,
            'rmse': rmse,
            'intercept': intercept,
            'feature_groups': {
                'media': used_media,
                'price': used_price,
                'seasonal': used_seasonal,
                'other': used_other
            },
            'abc_transformations': len([f for f in features if '_abc' in f])
        }
        clean_data['week'] = train_data['week']
        if not data_to_plot.empty:
            data_to_plot = data_to_plot[features + [target_col] + ['week']].dropna()
            clean_data = pd.concat([clean_data, data_to_plot])
        model_data = {
            'model': model,
            'features': features,
            'params': params,
            'transformed_data': clean_data,
            'coefficients_df': self.coef,
            'metadata': metadata,
            'abc_params': abc_params
        }
        result_predict = self.predict_with_model(model_data, forecast_data)
        if save_path:
            with open(save_path, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        return model_data, result_predict

    def plot_real_contribution_over_time(self, df_data, model, save_path='real_contribution.png'):
        """
        –†–µ–∞–ª—å–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–æ–¥–∞–∂ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        """
        import matplotlib.pyplot as plt
        import matplotlib.cm as cm
        import numpy as np
        from matplotlib.ticker import FuncFormatter

        features = self.features

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏—á–∏
        missing_features = [f for f in features if f not in df_data.columns]
        if missing_features:
            print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏: {missing_features}")
            return

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X = df_data[features].values
        coefficients = model.coef_
        intercept = model.intercept_

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∫–ª–∞–¥—ã
        contributions = {}

        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
        contributions['–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞'] = np.full(len(df_data), intercept)

        # –í–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π —Ñ–∏—á–∏
        for i, feature in enumerate(features):
            feature_contribution = df_data[feature].values * coefficients[i]
            clean_name = feature.replace('_abc', '').replace('_', ' ').title()
            contributions[clean_name] = feature_contribution
            print(f"{clean_name}: avg={np.mean(feature_contribution):.2f}, "
                  f"min={np.min(feature_contribution):.2f}, max={np.max(feature_contribution):.2f}")

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å
        if 'Week' in df_data.columns:
            x = pd.to_datetime(df_data['Week']) if df_data['Week'].dtype == 'object' else df_data['Week']
            x_label = '–î–∞—Ç–∞'
        elif 'week' in df_data.columns:
            x = df_data['week']
            x_label = '–ù–µ–¥–µ–ª–∏'
        else:
            x = range(len(df_data))
            x_label = '–ü–µ—Ä–∏–æ–¥'

        positive_contributions = {}
        negative_contributions = {}

        for name, values in contributions.items():
            if np.mean(values) >= 0:
                positive_contributions[name] = np.maximum(values, 0)  # —É–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã
            else:
                negative_contributions[name] = np.minimum(values, 0)  # —É–±–∏—Ä–∞–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã

        n_factors = len(contributions)
        colors_positive = cm.Set3(np.linspace(0, 1, len(positive_contributions)))
        colors_negative = cm.Set1(np.linspace(0, 1, len(negative_contributions)))

        fig, ax = plt.subplots(1, 1, figsize=(16, 10), dpi=100)

        if positive_contributions:
            pos_labels = list(positive_contributions.keys())
            pos_data = list(positive_contributions.values())
            ax.stackplot(x, *pos_data, labels=pos_labels, colors=colors_positive, alpha=0.8)

        if negative_contributions:
            neg_labels = list(negative_contributions.keys())
            neg_data = list(negative_contributions.values())
            ax.stackplot(x, *neg_data, labels=neg_labels, colors=colors_negative, alpha=0.8)

        if 'sales' in df_data.columns:
            actual_sales = df_data['sales'].values
            predicted_sales = sum(contributions.values())
            ax.plot(x, actual_sales, 'k-', linewidth=3, label='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏', alpha=0.9)
            ax.plot(x, predicted_sales, 'r--', linewidth=2, label='–ú–æ–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑', alpha=0.9)

            # –°—á–∏—Ç–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            r2 = np.corrcoef(actual_sales, predicted_sales)[0, 1] ** 2
            rmse = np.sqrt(np.mean((actual_sales - predicted_sales) ** 2))
            print(f"R¬≤: {r2:.3f}, RMSE: {rmse:.0f}")

        ax.set_title('–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–æ–¥–∞–∂ –ø–æ —Ñ–∞–∫—Ç–æ—Ä–∞–º –≤–æ –≤—Ä–µ–º–µ–Ω–∏\n(–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –≤–∫–ª–∞–¥—ã)',
                     fontsize=16, fontweight='bold')
        ax.set_xlabel(x_label, fontsize=12)
        ax.set_ylabel('–ü—Ä–æ–¥–∞–∂–∏', fontsize=12)

        ax.legend(fontsize=9, ncol=2, loc='upper left', bbox_to_anchor=(0, 1))

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å–µ–π
        if hasattr(x, 'dt'):  # –µ—Å–ª–∏ —ç—Ç–æ –¥–∞—Ç—ã
            ax.tick_params(axis='x', rotation=45)
        elif len(x) > 20:
            step = len(x) // 10
            plt.xticks(x[::step], rotation=45)

        ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{x:,.0f}'))

        # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è –Ω–∞ 0
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=0.5)

        # –°—Ç–∏–ª—å
        ax.spines["top"].set_alpha(0)
        ax.spines["right"].set_alpha(0)
        ax.spines["bottom"].set_alpha(0.3)
        ax.spines["left"].set_alpha(0.3)
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∫–ª–∞–¥–æ–≤
        print(f"\nüìä –°–†–ï–î–ù–ò–ï –í–ö–õ–ê–î–´ –ü–û –§–ê–ö–¢–û–†–ê–ú:")
        print("=" * 50)

        total_contributions = []
        for name, values in contributions.items():
            avg_contribution = np.mean(values)
            total_contributions.append((name, avg_contribution))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∫–ª–∞–¥–∞
        total_contributions.sort(key=lambda x: abs(x[1]), reverse=True)

        total = 0
        for name, avg_contribution in total_contributions:
            print(f"{name:<25}: {avg_contribution:>10.0f}")
            total += avg_contribution

        print("=" * 50)
        print(f"{'–ò–¢–û–ì–û':<25}: {total:>10.0f}")

        if 'sales' in df_data.columns:
            actual_avg = np.mean(df_data['sales'])
            print(f"{'–§–ê–ö–¢ (—Å—Ä–µ–¥–Ω–∏–π)':<25}: {actual_avg:>10.0f}")
            print(f"{'–†–ê–ó–ù–ò–¶–ê':<25}: {total - actual_avg:>10.0f}")

    def plot_feature_elasticity(self, model, transformed_data, save_path='feature_elasticity.png'):
        """
        –ì—Ä–∞—Ñ–∏–∫ —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (% –∏–∑–º–µ–Ω–µ–Ω–∏–µ Y –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ X –Ω–∞ 1%)
        """
        features = self.features
        coefficients = model.coef_

        elasticities = []

        target_col = self.target_col if hasattr(self, 'target_col') else 'sales'
        mean_y = np.mean(transformed_data[target_col])

        for i, feature in enumerate(features):
            if feature in transformed_data.columns:
                coef = coefficients[i]
                mean_x = np.mean(transformed_data[feature])

                # –≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å = (dY/dX) * (X/Y) = coef * (mean_X / mean_Y)
                if mean_y != 0 and mean_x != 0:
                    elasticity = coef * (mean_x / mean_y)

                    clean_name = feature.replace('_abc', '').replace('_', ' ').title()
                    elasticities.append((clean_name, elasticity, coef))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        elasticities.sort(key=lambda x: abs(x[1]), reverse=True)

        names = [x[0] for x in elasticities]
        elast_values = [x[1] for x in elasticities]
        colors = ['tab:green' if v > 0 else 'tab:red' for v in elast_values]

        fig, ax = plt.subplots(1, 1, figsize=(12, 8), dpi=80)

        bars = ax.barh(names, [abs(v) for v in elast_values], color=colors, alpha=0.8)

        for i, (bar, val) in enumerate(zip(bars, elast_values)):
            width = bar.get_width()
            ax.text(width + max([abs(v) for v in elast_values]) * 0.01,
                    bar.get_y() + bar.get_height() / 2,
                    f'{val:+.3f}', ha='left', va='center', fontweight='bold')

        ax.set_title('–≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ—Ä–æ–≤\n(% –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞ 1%)',
                     fontsize=16, fontweight='bold')
        ax.set_xlabel('|–≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å|')

        ax.spines["top"].set_alpha(0)
        ax.spines["bottom"].set_alpha(.3)
        ax.spines["right"].set_alpha(0)
        ax.spines["left"].set_alpha(.3)
        ax.grid(True, alpha=0.3, axis='x')

        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='tab:green', label='–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å'),
            Patch(facecolor='tab:red', label='–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å')
        ]
        ax.legend(handles=legend_elements, loc='lower right')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        return elasticities

    @staticmethod
    def plot_train_vs_forecast(model_data, forecast_data, predictions, target_col='sales'):
        """
        –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç –Ω–∞ train + –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ test
        """
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates

        # –ü–æ–ª—É—á–∞–µ–º train –¥–∞–Ω–Ω—ã–µ –∏–∑ model_data
        train_data = model_data['transformed_data']
        print(train_data.columns)
        if 'week' in train_data.columns:
            train_dates = pd.to_datetime(train_data['week'])
            train_actual = train_data[target_col].values
        elif hasattr(train_data.index, 'to_pydatetime'):  # –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å - –¥–∞—Ç—ã
            train_dates = train_data.index
            train_actual = train_data[target_col].values
        else:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–∞–º–∏ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏")
            return

        if 'week' in forecast_data.columns:
            test_dates = pd.to_datetime(forecast_data['week'])
        elif hasattr(forecast_data.index, 'to_pydatetime'):
            test_dates = forecast_data.index
        else:
            test_dates = pd.date_range(start=train_dates.max(), periods=len(predictions) + 1, freq='W')[1:]

        fig, ax = plt.subplots(figsize=(15, 8))

        ax.plot(train_dates, train_actual, 'b-', linewidth=2, label='–§–∞–∫—Ç (Train)', alpha=0.8)
        ax.plot(test_dates, predictions, 'r--', linewidth=2, label='–ü—Ä–æ–≥–Ω–æ–∑ (Test)', alpha=0.8)

        if len(train_dates) > 0 and len(test_dates) > 0:
            split_date = train_dates.max()
            ax.axvline(x=split_date, color='gray', linestyle=':', alpha=0.7,
                       label=f'Train/Test split ({split_date.strftime("%Y-%m-%d")})')

        ax.set_xlabel('–î–∞—Ç–∞')
        ax.set_ylabel(target_col.title())
        ax.set_title('–§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑: Train + Test –¥–∞–Ω–Ω—ã–µ')
        ax.legend()
        ax.grid(True, alpha=0.3)

        if len(train_dates) + len(test_dates) > 52:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –≥–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö
            ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # –ö–∞–∂–¥—ã–µ 3 –º–µ—Å—è—Ü–∞
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        else:
            ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=4))  # –ö–∞–∂–¥—ã–µ 4 –Ω–µ–¥–µ–ª–∏
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))

        plt.xticks(rotation=45)
        plt.tight_layout()

        print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞:")
        print(f"Train –ø–µ—Ä–∏–æ–¥: {train_dates.min().strftime('%Y-%m-%d')} - {train_dates.max().strftime('%Y-%m-%d')}")
        print(f"Test –ø–µ—Ä–∏–æ–¥: {test_dates.min().strftime('%Y-%m-%d')} - {test_dates.max().strftime('%Y-%m-%d')}")
        print(f"–°—Ä–µ–¥–Ω–∏–π —Ñ–∞–∫—Ç (train): {np.mean(train_actual):.2f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ–≥–Ω–æ–∑ (test): {np.mean(predictions):.2f}")
        print(f"–†–∞–∑–Ω–∏—Ü–∞ —Å—Ä–µ–¥–Ω–∏—Ö: {np.mean(predictions) - np.mean(train_actual):.2f}")

        plt.show()

    def get_real_media_levels(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ï —É—Ä–æ–≤–Ω–∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        if self.train_data is None:
            return {}

        real_data = {}

        # –ú–∞–ø–ø–∏–Ω–≥ ABC –∫–∞–Ω–∞–ª–æ–≤ –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º –≤ —Ç–≤–æ–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        channel_mapping = {
            'federal_tv': 'federal_tv',
            'thematic_tv': 'thematic_tv',
            'regional_tv': 'regional_tv',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç1_—Ç–≤_—Ä–µ–π—Ç–∏–Ω–≥–∏',  # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç2_—Ç–≤_—Ä–µ–π—Ç–∏–Ω–≥–∏',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç3_—Ç–≤_—Ä–µ–π—Ç–∏–Ω–≥–∏',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç4_—Ç–≤_—Ä–µ–π—Ç–∏–Ω–≥–∏',
        }

        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ train_data:")
        print([col for col in self.train_data.columns if
               any(x in col.lower() for x in ['federal', 'thematic', 'regional', '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç'])])

        for abc_name, original_name in channel_mapping.items():
            if original_name in self.train_data.columns:
                data = self.train_data[original_name].dropna()
                if len(data) > 0:
                    real_data[abc_name] = {
                        'min': float(data.min()),
                        'max': float(data.max()),
                        'mean': float(data.mean()),
                        'current': float(data.iloc[-1]),  # –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                        'q75': float(data.quantile(0.75)),  # 75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å
                    }
                    print(
                        f"{abc_name}: min={real_data[abc_name]['min']:.1f}, max={real_data[abc_name]['max']:.1f}, current={real_data[abc_name]['current']:.1f}")
            else:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {original_name}")

        return real_data

    def map_abc_to_original_channel(self, abc_channel_name):
        """–ú–∞–ø–ø–∏–Ω–≥ ABC –∏–º–µ–Ω–∏ –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É"""
        mapping = {
            'federal_tv': 'federal_tv',
            'thematic_tv': 'thematic_tv',
            'regional_tv': 'regional_tv',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏',
            '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏': '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏',
        }
        return mapping.get(abc_channel_name, abc_channel_name)

    def predict_with_model(self, model_data, new_data, target_col='sales'):
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ —Å —Å–æ–∑–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é

        Args:
            model_data: —Ä–µ–∑—É–ª—å—Ç–∞—Ç create_model_from_coefficients
            new_data: –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (DataFrame)
            target_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

        Returns:
            dict: –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """

        print(" –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—å—é...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏
        model = model_data['model']
        features = model_data['features']
        abc_params = model_data['abc_params']

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        forecast_data = new_data.copy()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        missing_features = [f for f in features if f not in forecast_data.columns]

        if missing_features:

            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            available_features = [f for f in features if f in forecast_data.columns]
            if len(available_features) < len(features) * 0.5:  # –ú–µ–Ω—å—à–µ 50% –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                return None

            print(f"Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å {len(available_features)}/{len(features)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
            # –ù—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            features = available_features

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            available_indices = [i for i, feature in enumerate(self.features) if feature in available_features]

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            filtered_coefficients = [self.coef[i] for i in available_indices]

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            model = LinearRegression()
            model.coef_ = filtered_coefficients
            model.intercept_ = model_data['model'].intercept_  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ intercept
            model.n_features_in_ = len(features)
            model.feature_names_in_ = np.array(features)

        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0 (–∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏)
        numeric_columns = forecast_data.select_dtypes(include=[np.number]).columns
        forecast_data[numeric_columns] = forecast_data[numeric_columns].fillna(0)
        forecast_data[numeric_columns] = forecast_data[numeric_columns].replace([np.inf, -np.inf], 0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        try:
            X_forecast = forecast_data[features].values
        except KeyError as e:
            return None

        print(f" –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {len(X_forecast)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π...")

        try:
            predictions = model.predict(X_forecast)
        except Exception as e:
            return None

        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'predictions': predictions,
            # 'forecast_data': forecast_data,
            'used_features': features,
            'missing_features': missing_features,
            'n_predictions': len(predictions),
            'prediction_stats': {
                'mean': np.mean(predictions),
                'std': np.std(predictions),
                'min': np.min(predictions),
                'max': np.max(predictions)
            }
        }

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        if target_col in forecast_data.columns and any(forecast_data[target_col]):
            actual = forecast_data[target_col].values

            mse = mean_squared_error(actual, predictions)
            r2 = r2_score(actual, predictions)
            print('–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:')
            print(actual, predictions)
            print('rmse', np.sqrt(mse))
            print('r2', r2)
        else:
            print('kuku')
            self.plot_train_vs_forecast(model_data, forecast_data, predictions, target_col)
        # print(forecast_data, predictions)
        # self.save_forecast_results(result)
        return result


model_coef = [475.47188011388477, 655.0730197811116, 2103.7308818153565, -237.91232243849566, -1014.3196438689523, 539.3589663504629, 2816.363174461569, -4822.073524769513, -21.195168030529913, -376.63058882530095, 114.72658571909479, 283.0991711367753, -38.63932553007724]

model_features = ['federal_tv_abc', 'thematic_tv_abc', 'regional_tv_abc', '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_abc', '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_abc', '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_abc', '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_abc', 'price_ratio', 'is_spring', 'is_summer', 'is_autumn', 'is_winter', 'competitor_price_trend']

# –ü–†–ê–í–ò–õ–¨–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–≤–æ–µ–π —Ñ—É–Ω–∫—Ü–∏–∏:
model_abc_params = {'federal_tv_A': 0.8578554509227851, 'federal_tv_B': 4.978030195753396, 'federal_tv_C': 4.744117793338168, 'thematic_tv_A': 0.8697071021624991, 'thematic_tv_B': 3.2613417490966987, 'thematic_tv_C': 0.37028284391806165, 'regional_tv_A': 0.7534743099591871, 'regional_tv_B': 0.808453454643836, 'regional_tv_C': 5.833099501032512, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_A': 0.5550793311512732, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_B': 1.6000699816692099, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç1_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_C': 0.7746348671954331, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_A': 0.6513321011751504, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_B': 2.57521452253587, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç2_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_C': 6.445221410745203, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_A': 0.4249855292041599, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_B': 0.9675091228624451, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç3_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_C': 2.2449005988609114, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_A': 0.7849159268740338, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_B': 0.44380177532213994, '–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç4_–¢–í_–†–µ–π—Ç–∏–Ω–≥–∏_C': 5.630751549543227}

# params = pd.read_excel('model_params.xlsx')

data = pd.read_csv('data.csv', sep=';', encoding='utf-8')
data['Week'] = pd.to_datetime(data['Week'], format='%d.%m.%Y')

data = create_all_features(data.copy())
normalized_columns = list(map(lambda x: normalize_feature_name(x), data.columns))
data.columns = normalized_columns
# print(data.columns, 'qwsd')
train_end = pd.to_datetime('2012-09-24')
start_forecast = pd.to_datetime('2012-12-30')
forecast_end = pd.to_datetime('2013-12-30')

dict_params, data = add_abc(model_abc_params, data)

if train_end != start_forecast:
    data_to_plot = data[(data['week'] > train_end) & (data['week'] <= start_forecast)].copy()
else:
    data_to_plot = pd.DataFrame()

data.to_csv('data_abc.csv', index=False)

train_data = data[data['week'] <= train_end].copy()
# print(train_data['federal_tv_abc'])

forecast_data = data[(data['week'] > start_forecast) & (data['week'] <= forecast_end)].copy()

visualizer = MMMVisualizer(model_coef, model_features, model_abc_params, train_data=train_data)

model, result_predict = visualizer.create_model_from_coefficients(train_data, forecast_data, dict_params, data_to_plot)

print(result_predict)
# –°—Ç—Ä–æ–∏–º –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏

visualizer.plot_saturation_curves()
visualizer.plot_real_contribution_over_time(train_data, model['model'])
# visualizer.plot_feature_elasticity(model['model'], train_data)

# print(model)
