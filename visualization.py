import pickle
from datetime import datetime
from sklearn.linear_model import LinearRegression

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from matplotlib.ticker import FuncFormatter
from pprint import pprint
from sklearn.metrics import r2_score, mean_squared_error

from main import create_all_features, abc_transform, append_to_csv

plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")


def add_abc(abc_params_dict, data_wo_abc):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç ABC –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º
    abc_params_dict: —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–∞ {'channel_A': value, 'channel_B': value, 'channel_C': value}
    data_wo_abc: DataFrame —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    channels = set()
    for key in abc_params_dict.keys():
        if key.endswith(('_A', '_B', '_C')):
            channel = key.rsplit('_', 1)[0]  # –£–±–∏—Ä–∞–µ–º _A, _B, _C
            channels.add(channel)

    applied_params = {}

    for channel in channels:
        a_key = f'{channel}_A'
        b_key = f'{channel}_B'
        c_key = f'{channel}_C'

        if all(key in abc_params_dict for key in [a_key, b_key, c_key]):
            A = abc_params_dict[a_key]
            B = abc_params_dict[b_key]
            C = abc_params_dict[c_key]

            print(f'{channel}_abc', 'qq')

            # –ò—â–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–ª–æ–Ω–∫—É (—É–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã)
            original_feature = channel.lower()

            if original_feature in data_wo_abc.columns:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º ABC –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
                original_values = data_wo_abc[original_feature].values
                abc_transformed = abc_transform(original_values, A, B, C)
                feature_name = f'{original_feature}_abc'
                print(feature_name, 'feature_name')
                data_wo_abc[feature_name] = abc_transformed
                applied_params[a_key] = A
                applied_params[b_key] = B
                applied_params[c_key] = C

            else:
                print(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–∏—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {original_feature}")

    return applied_params, data_wo_abc


def normalize_feature_name(feature_name):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
    """
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –∑–∞–ø—è—Ç—ã–µ, —Ç–æ—á–∫–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = (feature_name.lower()
                  .replace(' ', '_')
                  .replace(',', '')
                  .replace('.', '')
                  .replace('(', '')
                  .replace(')', ''))
    return normalized


class MMMVisualizer:
    def __init__(self, coef, features, abc_params):
        """
        results_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ features, p-values, coef, A, B, C
        """
        self.coef = np.array(coef)
        features = list(map(lambda x: x.lower(), features))
        self.features = features
        self.abc_params = abc_params

    def plot_saturation_curves(self, save_path='saturation_curves.png'):
        """
        –°—Ç—Ä–æ–∏—Ç –∫—Ä–∏–≤—ã–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ–¥–∏–∞-–∫–∞–Ω–∞–ª–∞
        model_abc_params: —Å–ª–æ–≤–∞—Ä—å —Å ABC –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≤–∏–¥–∞ {'channel_A': value, 'channel_B': value, 'channel_C': value}
        """
        model_abc_params = self.abc_params
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –∏–∑ –∫–ª—é—á–µ–π
        channels = set()
        for key in model_abc_params.keys():
            if key.endswith(('_A', '_B', '_C')):
                channel = key.rsplit('_', 1)[0]  # –£–±–∏—Ä–∞–µ–º _A, _B, _C
                channels.add(channel)

        media_channels = []
        for channel in channels:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ —Ç—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
            a_key = f'{channel}_A'
            b_key = f'{channel}_B'
            c_key = f'{channel}_C'

            if all(key in model_abc_params for key in [a_key, b_key, c_key]):
                media_channels.append({
                    'name': channel,
                    'A': model_abc_params[a_key],
                    'B': model_abc_params[b_key],
                    'C': model_abc_params[c_key]
                })

        n_channels = len(media_channels)
        if n_channels == 0:
            print("–ù–µ—Ç –º–µ–¥–∏–∞-–∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return

        cols = min(3, n_channels)
        rows = (n_channels + cols - 1) // cols

        fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))

        if n_channels == 1:
            axes = [axes]
        elif rows == 1:
            axes = [axes] if cols == 1 else list(axes)
        else:
            axes = axes.flatten()

        fig.suptitle('–ö—Ä–∏–≤—ã–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è –º–µ–¥–∏–∞-–∫–∞–Ω–∞–ª–æ–≤', fontsize=16, fontweight='bold')

        for idx, channel_data in enumerate(media_channels):
            ax = axes[idx]

            A = channel_data['A']
            B = channel_data['B']
            C = channel_data['C']
            channel_name = channel_data['name'].replace('_', ' ').title()

            max_spend = 1000
            spend_range = np.linspace(0, max_spend, 200)

            def abc_curve(spend, a, b, c):
                scaled = b * spend
                saturated = (c * scaled) / (1 + c * scaled)
                return saturated

            response = abc_curve(spend_range, A, B, C)

            ax.plot(spend_range, response, linewidth=3, color='#2E86AB', label='–ö—Ä–∏–≤–∞—è –æ—Ç–∫–ª–∏–∫–∞')
            ax.fill_between(spend_range, 0, response, alpha=0.3, color='#2E86AB')

            current_spend = 500
            current_response = abc_curve(current_spend, A, B, C)
            ax.scatter([current_spend], [current_response], color='red', s=100,
                       zorder=5, label='–¢–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å')

            ax.set_title(f'{channel_name}\nA={A:.3f}, B={B:.3f}, C={C:.3f}',
                         fontweight='bold', pad=20)
            ax.set_xlabel('–ú–µ–¥–∏–∞-–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', fontweight='bold')
            ax.set_ylabel('–ü—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–¥–∞–∂', fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.legend()

            if C < 1:
                saturation_text = "–ë—ã—Å—Ç—Ä–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ"
            elif C < 2:
                saturation_text = "–£–º–µ—Ä–µ–Ω–Ω–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ"
            else:
                saturation_text = "–ú–µ–¥–ª–µ–Ω–Ω–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ"

            ax.text(0.05, 0.95, saturation_text, transform=ax.transAxes,
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                    verticalalignment='top', fontsize=9)

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∏
        for idx in range(n_channels, len(axes)):
            axes[idx].remove()

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # plot_saturation_curves(model_abc_params)

    def create_model_from_coefficients(self, train_data, forecast_data, abc_params, data_to_plot,
                                       target_col='sales', intercept=None, save_path=None):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∏ ABC –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Args:
            original_data: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (DataFrame)
            coefficients_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ features, p-values, coef, A, B, C
            target_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            intercept: —Å–≤–æ–±–æ–¥–Ω—ã–π —á–ª–µ–Ω (–µ—Å–ª–∏ None, —Ä–∞—Å—Å—á–∏—Ç–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            dict: model_data —Å –º–æ–¥–µ–ª—å—é, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        features = self.features
        print(train_data.columns)
        print(features)
        missing_features = [f for f in features if f not in train_data.columns]

        if missing_features:
            print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
            return None

        clean_data = train_data[features + [target_col]].dropna()

        clean_data.to_csv('test.csv', index=False)

        X = clean_data[features].values
        y = clean_data[target_col].values
        coefficients = self.coef

        print(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(clean_data)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

        model = LinearRegression()
        model.coef_ = coefficients
        if intercept is None:
            predicted_without_intercept = X @ coefficients
            intercept = np.mean(y) - np.mean(predicted_without_intercept)

        model.intercept_ = intercept
        model.n_features_in_ = len(features)
        model.feature_names_in_ = np.array(features)

        y_pred = model.predict(X)
        r2_score_ = r2_score(y, y_pred)

        mse = mean_squared_error(y, y_pred)
        rmse = np.sqrt(mse)

        print(f"–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   R¬≤: {r2_score_:.4f}")
        print(f"   RMSE: {rmse:.2f}")

        params = abc_params.copy()
        used_media = [f for f in features if '_abc' in f]
        used_price = [f for f in features if any(p in f.lower() for p in ['price', 'premium', 'discount'])]
        used_seasonal = [f for f in features if
                         any(s in f.lower() for s in ['spring', 'summer', 'autumn', 'winter', 'holiday', 'trend'])]
        used_other = [f for f in features if f not in used_media + used_price + used_seasonal]
        params.update({
            'used_media': used_media,
            'used_price': used_price,
            'used_seasonal': used_seasonal,
            'used_other': used_other,
            'intercept': intercept
        })

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metadata = {
            'timestamp': timestamp,
            'creation_method': 'from_coefficients',
            'n_features': len(features),
            'target_col': target_col,
            'train_data_shape': clean_data.shape,
            'r2_score': r2_score_,
            'rmse': rmse,
            'intercept': intercept,
            'feature_groups': {
                'media': used_media,
                'price': used_price,
                'seasonal': used_seasonal,
                'other': used_other
            },
            'abc_transformations': len([f for f in features if '_abc' in f])
        }
        clean_data['week'] = train_data['week']
        if not data_to_plot.empty:
            data_to_plot = data_to_plot[features + [target_col] + ['week']].dropna()
            clean_data = pd.concat([clean_data, data_to_plot])
        model_data = {
            'model': model,
            'features': features,
            'params': params,
            'transformed_data': clean_data,
            'coefficients_df': self.coef,
            'metadata': metadata,
            'abc_params': abc_params
        }
        result_predict = self.predict_with_model(model_data, forecast_data)
        if save_path:
            with open(save_path, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        return model_data, result_predict

    def plot_real_contribution_over_time(self, df_data, model, save_path='real_contribution.png'):
        """
        –†–µ–∞–ª—å–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–æ–¥–∞–∂ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        """
        import matplotlib.pyplot as plt
        import matplotlib.cm as cm
        import numpy as np
        from matplotlib.ticker import FuncFormatter

        features = self.features

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏—á–∏
        missing_features = [f for f in features if f not in df_data.columns]
        if missing_features:
            print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏: {missing_features}")
            return

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X = df_data[features].values
        coefficients = model.coef_
        intercept = model.intercept_

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∫–ª–∞–¥—ã
        contributions = {}

        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
        contributions['–ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞'] = np.full(len(df_data), intercept)

        # –í–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π —Ñ–∏—á–∏
        for i, feature in enumerate(features):
            feature_contribution = df_data[feature].values * coefficients[i]
            clean_name = feature.replace('_abc', '').replace('_', ' ').title()
            contributions[clean_name] = feature_contribution
            print(f"{clean_name}: avg={np.mean(feature_contribution):.2f}, "
                  f"min={np.min(feature_contribution):.2f}, max={np.max(feature_contribution):.2f}")

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å
        if 'Week' in df_data.columns:
            x = pd.to_datetime(df_data['Week']) if df_data['Week'].dtype == 'object' else df_data['Week']
            x_label = '–î–∞—Ç–∞'
        elif 'week' in df_data.columns:
            x = df_data['week']
            x_label = '–ù–µ–¥–µ–ª–∏'
        else:
            x = range(len(df_data))
            x_label = '–ü–µ—Ä–∏–æ–¥'

        positive_contributions = {}
        negative_contributions = {}

        for name, values in contributions.items():
            if np.mean(values) >= 0:
                positive_contributions[name] = np.maximum(values, 0)  # —É–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã
            else:
                negative_contributions[name] = np.minimum(values, 0)  # —É–±–∏—Ä–∞–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã

        n_factors = len(contributions)
        colors_positive = cm.Set3(np.linspace(0, 1, len(positive_contributions)))
        colors_negative = cm.Set1(np.linspace(0, 1, len(negative_contributions)))

        fig, ax = plt.subplots(1, 1, figsize=(16, 10), dpi=100)

        if positive_contributions:
            pos_labels = list(positive_contributions.keys())
            pos_data = list(positive_contributions.values())
            ax.stackplot(x, *pos_data, labels=pos_labels, colors=colors_positive, alpha=0.8)

        if negative_contributions:
            neg_labels = list(negative_contributions.keys())
            neg_data = list(negative_contributions.values())
            ax.stackplot(x, *neg_data, labels=neg_labels, colors=colors_negative, alpha=0.8)

        if 'sales' in df_data.columns:
            actual_sales = df_data['sales'].values
            predicted_sales = sum(contributions.values())
            ax.plot(x, actual_sales, 'k-', linewidth=3, label='–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–¥–∞–∂–∏', alpha=0.9)
            ax.plot(x, predicted_sales, 'r--', linewidth=2, label='–ú–æ–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑', alpha=0.9)

            # –°—á–∏—Ç–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            r2 = np.corrcoef(actual_sales, predicted_sales)[0, 1] ** 2
            rmse = np.sqrt(np.mean((actual_sales - predicted_sales) ** 2))
            print(f"R¬≤: {r2:.3f}, RMSE: {rmse:.0f}")

        ax.set_title('–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –ø—Ä–æ–¥–∞–∂ –ø–æ —Ñ–∞–∫—Ç–æ—Ä–∞–º –≤–æ –≤—Ä–µ–º–µ–Ω–∏\n(–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –≤–∫–ª–∞–¥—ã)',
                     fontsize=16, fontweight='bold')
        ax.set_xlabel(x_label, fontsize=12)
        ax.set_ylabel('–ü—Ä–æ–¥–∞–∂–∏', fontsize=12)

        ax.legend(fontsize=9, ncol=2, loc='upper left', bbox_to_anchor=(0, 1))

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å–µ–π
        if hasattr(x, 'dt'):  # –µ—Å–ª–∏ —ç—Ç–æ –¥–∞—Ç—ã
            ax.tick_params(axis='x', rotation=45)
        elif len(x) > 20:
            step = len(x) // 10
            plt.xticks(x[::step], rotation=45)

        ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{x:,.0f}'))

        # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è –Ω–∞ 0
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=0.5)

        # –°—Ç–∏–ª—å
        ax.spines["top"].set_alpha(0)
        ax.spines["right"].set_alpha(0)
        ax.spines["bottom"].set_alpha(0.3)
        ax.spines["left"].set_alpha(0.3)
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∫–ª–∞–¥–æ–≤
        print(f"\nüìä –°–†–ï–î–ù–ò–ï –í–ö–õ–ê–î–´ –ü–û –§–ê–ö–¢–û–†–ê–ú:")
        print("=" * 50)

        total_contributions = []
        for name, values in contributions.items():
            avg_contribution = np.mean(values)
            total_contributions.append((name, avg_contribution))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∫–ª–∞–¥–∞
        total_contributions.sort(key=lambda x: abs(x[1]), reverse=True)

        total = 0
        for name, avg_contribution in total_contributions:
            print(f"{name:<25}: {avg_contribution:>10.0f}")
            total += avg_contribution

        print("=" * 50)
        print(f"{'–ò–¢–û–ì–û':<25}: {total:>10.0f}")

        if 'sales' in df_data.columns:
            actual_avg = np.mean(df_data['sales'])
            print(f"{'–§–ê–ö–¢ (—Å—Ä–µ–¥–Ω–∏–π)':<25}: {actual_avg:>10.0f}")
            print(f"{'–†–ê–ó–ù–ò–¶–ê':<25}: {total - actual_avg:>10.0f}")

    def plot_feature_elasticity(self, model, transformed_data, save_path='feature_elasticity.png'):
        """
        –ì—Ä–∞—Ñ–∏–∫ —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ (% –∏–∑–º–µ–Ω–µ–Ω–∏–µ Y –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ X –Ω–∞ 1%)
        """
        features = self.features
        coefficients = model.coef_

        elasticities = []

        target_col = self.target_col if hasattr(self, 'target_col') else 'sales'
        mean_y = np.mean(transformed_data[target_col])

        for i, feature in enumerate(features):
            if feature in transformed_data.columns:
                coef = coefficients[i]
                mean_x = np.mean(transformed_data[feature])

                # –≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å = (dY/dX) * (X/Y) = coef * (mean_X / mean_Y)
                if mean_y != 0 and mean_x != 0:
                    elasticity = coef * (mean_x / mean_y)

                    clean_name = feature.replace('_abc', '').replace('_', ' ').title()
                    elasticities.append((clean_name, elasticity, coef))

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        elasticities.sort(key=lambda x: abs(x[1]), reverse=True)

        names = [x[0] for x in elasticities]
        elast_values = [x[1] for x in elasticities]
        colors = ['tab:green' if v > 0 else 'tab:red' for v in elast_values]

        fig, ax = plt.subplots(1, 1, figsize=(12, 8), dpi=80)

        bars = ax.barh(names, [abs(v) for v in elast_values], color=colors, alpha=0.8)

        for i, (bar, val) in enumerate(zip(bars, elast_values)):
            width = bar.get_width()
            ax.text(width + max([abs(v) for v in elast_values]) * 0.01,
                    bar.get_y() + bar.get_height() / 2,
                    f'{val:+.3f}', ha='left', va='center', fontweight='bold')

        ax.set_title('–≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ—Ä–æ–≤\n(% –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ñ–∞–∫—Ç–æ—Ä–∞ –Ω–∞ 1%)',
                     fontsize=16, fontweight='bold')
        ax.set_xlabel('|–≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å|')

        ax.spines["top"].set_alpha(0)
        ax.spines["bottom"].set_alpha(.3)
        ax.spines["right"].set_alpha(0)
        ax.spines["left"].set_alpha(.3)
        ax.grid(True, alpha=0.3, axis='x')

        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='tab:green', label='–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å'),
            Patch(facecolor='tab:red', label='–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å')
        ]
        ax.legend(handles=legend_elements, loc='lower right')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        return elasticities

    @staticmethod
    def plot_train_vs_forecast(model_data, forecast_data, predictions, target_col='sales'):
        """
        –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç –Ω–∞ train + –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ test
        """
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates

        # –ü–æ–ª—É—á–∞–µ–º train –¥–∞–Ω–Ω—ã–µ –∏–∑ model_data
        train_data = model_data['transformed_data']
        print(train_data.columns)
        if 'week' in train_data.columns:
            train_dates = pd.to_datetime(train_data['week'])
            train_actual = train_data[target_col].values
        elif hasattr(train_data.index, 'to_pydatetime'):  # –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å - –¥–∞—Ç—ã
            train_dates = train_data.index
            train_actual = train_data[target_col].values
        else:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –¥–∞—Ç–∞–º–∏ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏")
            return

        if 'week' in forecast_data.columns:
            test_dates = pd.to_datetime(forecast_data['week'])
        elif hasattr(forecast_data.index, 'to_pydatetime'):
            test_dates = forecast_data.index
        else:
            test_dates = pd.date_range(start=train_dates.max(), periods=len(predictions) + 1, freq='W')[1:]

        fig, ax = plt.subplots(figsize=(15, 8))

        ax.plot(train_dates, train_actual, 'b-', linewidth=2, label='–§–∞–∫—Ç (Train)', alpha=0.8)
        ax.plot(test_dates, predictions, 'r--', linewidth=2, label='–ü—Ä–æ–≥–Ω–æ–∑ (Test)', alpha=0.8)

        if len(train_dates) > 0 and len(test_dates) > 0:
            split_date = train_dates.max()
            ax.axvline(x=split_date, color='gray', linestyle=':', alpha=0.7,
                       label=f'Train/Test split ({split_date.strftime("%Y-%m-%d")})')

        ax.set_xlabel('–î–∞—Ç–∞')
        ax.set_ylabel(target_col.title())
        ax.set_title('–§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑: Train + Test –¥–∞–Ω–Ω—ã–µ')
        ax.legend()
        ax.grid(True, alpha=0.3)

        if len(train_dates) + len(test_dates) > 52:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –≥–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö
            ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # –ö–∞–∂–¥—ã–µ 3 –º–µ—Å—è—Ü–∞
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
        else:
            ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=4))  # –ö–∞–∂–¥—ã–µ 4 –Ω–µ–¥–µ–ª–∏
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))

        plt.xticks(rotation=45)
        plt.tight_layout()

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞:")
        print(f"Train –ø–µ—Ä–∏–æ–¥: {train_dates.min().strftime('%Y-%m-%d')} - {train_dates.max().strftime('%Y-%m-%d')}")
        print(f"Test –ø–µ—Ä–∏–æ–¥: {test_dates.min().strftime('%Y-%m-%d')} - {test_dates.max().strftime('%Y-%m-%d')}")
        print(f"–°—Ä–µ–¥–Ω–∏–π —Ñ–∞–∫—Ç (train): {np.mean(train_actual):.2f}")
        print(f"–°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ–≥–Ω–æ–∑ (test): {np.mean(predictions):.2f}")
        print(f"–†–∞–∑–Ω–∏—Ü–∞ —Å—Ä–µ–¥–Ω–∏—Ö: {np.mean(predictions) - np.mean(train_actual):.2f}")

        plt.show()

    def predict_with_model(self, model_data, new_data, target_col='sales'):
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ —Å —Å–æ–∑–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é

        Args:
            model_data: —Ä–µ–∑—É–ª—å—Ç–∞—Ç create_model_from_coefficients
            new_data: –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (DataFrame)
            target_col: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

        Returns:
            dict: –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """

        print(" –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—å—é...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏
        model = model_data['model']
        features = model_data['features']
        abc_params = model_data['abc_params']

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        forecast_data = new_data.copy()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        missing_features = [f for f in features if f not in forecast_data.columns]

        if missing_features:

            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            available_features = [f for f in features if f in forecast_data.columns]
            if len(available_features) < len(features) * 0.5:  # –ú–µ–Ω—å—à–µ 50% –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                return None

            print(f"Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å {len(available_features)}/{len(features)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
            # –ù—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            features = available_features

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            available_indices = [i for i, feature in enumerate(self.features) if feature in available_features]

            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            filtered_coefficients = [self.coef[i] for i in available_indices]

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            model = LinearRegression()
            model.coef_ = filtered_coefficients
            model.intercept_ = model_data['model'].intercept_  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ intercept
            model.n_features_in_ = len(features)
            model.feature_names_in_ = np.array(features)

        # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0 (–∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏)
        numeric_columns = forecast_data.select_dtypes(include=[np.number]).columns
        forecast_data[numeric_columns] = forecast_data[numeric_columns].fillna(0)
        forecast_data[numeric_columns] = forecast_data[numeric_columns].replace([np.inf, -np.inf], 0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        try:
            X_forecast = forecast_data[features].values
        except KeyError as e:
            return None

        print(f" –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {len(X_forecast)} –Ω–∞–±–ª—é–¥–µ–Ω–∏–π...")

        try:
            predictions = model.predict(X_forecast)
        except Exception as e:
            return None

        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'predictions': predictions,
            # 'forecast_data': forecast_data,
            'used_features': features,
            'missing_features': missing_features,
            'n_predictions': len(predictions),
            'prediction_stats': {
                'mean': np.mean(predictions),
                'std': np.std(predictions),
                'min': np.min(predictions),
                'max': np.max(predictions)
            }
        }

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è - —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        if target_col in forecast_data.columns and any(forecast_data[target_col]):
            actual = forecast_data[target_col].values

            mse = mean_squared_error(actual, predictions)
            r2 = r2_score(actual, predictions)
            print('–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:')
            print(actual, predictions)
            print('rmse', np.sqrt(mse))
            print('r2', r2)
        else:
            print('kuku')
            self.plot_train_vs_forecast(model_data, forecast_data, predictions, target_col)
        # print(forecast_data, predictions)
        # self.save_forecast_results(result)
        return result


model_coef = [1795.849314337603, 3541.7628509147316, -141.0292659448475, 51.73630466732949]

model_features = ['federal_tv_abc', 'price_ratio_lag2', 'competitor_price_trend', 'avg_price_category']

model_abc_params = {'federal_tv_A': 0.897141172165341, 'federal_tv_B': 2.6468467079022626, 'federal_tv_C': 0.18102005465458929}

# params = pd.read_excel('model_params.xlsx')

data = pd.read_csv('data.csv', sep=';', encoding='utf-8')
data['Week'] = pd.to_datetime(data['Week'], format='%d.%m.%Y')

data = create_all_features(data.copy())
normalized_columns = list(map(lambda x: normalize_feature_name(x), data.columns))
data.columns = normalized_columns
# print(data.columns, 'qwsd')
train_end = pd.to_datetime('2012-06-30')
start_forecast = pd.to_datetime('2012-12-30')
forecast_end = pd.to_datetime('2013-12-30')

dict_params, data = add_abc(model_abc_params, data)

if train_end != start_forecast:
    data_to_plot = data[(data['week'] > train_end) & (data['week'] <= start_forecast)].copy()
else:
    data_to_plot = pd.DataFrame()

data.to_csv('data_abc.csv', index=False)

train_data = data[data['week'] <= train_end].copy()
print(train_data['federal_tv_abc'])

forecast_data = data[(data['week'] > start_forecast) & (data['week'] <= forecast_end)].copy()

visualizer = MMMVisualizer(model_coef, model_features, model_abc_params)

model, result_predict = visualizer.create_model_from_coefficients(train_data, forecast_data, dict_params, data_to_plot)

print(result_predict)
# –°—Ç—Ä–æ–∏–º –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏

# visualizer.plot_saturation_curves()
visualizer.plot_real_contribution_over_time(train_data, model['model'])
# visualizer.plot_feature_elasticity(model['model'], train_data)

# print(model)
